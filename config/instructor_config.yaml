# Model parameters
model_id: 'mistralai/Mistral-7B-v0.1'
max_length: 1024
hf_key_path: '../keys/hf_key.txt'

# Data parameters
instruction_data_path: './data/databricks-dolly-15k/databricks-dolly-15k.jsonl'
instruction_schema:
  user: 'instruction'
  context: 'context'
  assistant: 'response'

# LoRA parameters
r: 16
lora_alpha: 16
lora_dropout: 0.05
bias: "none"
task_type: "CAUSAL_LM"
target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"
  - "lm_head"

# Training parameters
output_dir: './models/instruction_tuned/'
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
gradient_checkpointing: True
learning_rate: 2.e-5
lr_scheduler_type: "constant"
max_steps: 200
logging_steps: 1
optim: "paged_adamw_32bit"
warmup_ratio: 0.03
report_to: []
