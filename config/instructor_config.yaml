# Model parameters
model_id: 'mistralai/Mistral-7B-v0.1'
quantization: True
max_length: 512
packing: False
sliding_window: 256
hf_key_path: '../keys/hf_key.txt'
checkpoint_name: checkpoint-225
model_repo_id: rjuggins/instruction_mistral_7b_v1_2_epochs_test
seed: 42
test_frac: 0.02

# Data parameters
instruction_data_path: './data/databricks-dolly-15k/databricks-dolly-15k.jsonl'
# instruction_data_path: './data/python_code_instructions_18k_alpaca/python_code_instructions_18k_alpaca.jsonl'
instruction_schema:
  user: 'instruction'
  context: 'context'
  assistant: 'response'
  # user: 'instruction'
  # context: 'input'
  # assistant: 'output'

bnb_params:
  load_in_4bit: True
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_use_double_quant: False

lora_params: 
  r: 32
  lora_alpha: 16
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
    - "lm_head"

training_params: 
  output_dir: "./models/instruction_tuned/"
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 4
  max_grad_norm: 0.3
  gradient_checkpointing: True
  learning_rate: 2.e-5
  lr_scheduler_type: "constant"
  # max_steps: 200
  num_train_epochs: 2
  fp16: False
  bf16: True
  group_by_length: True
  save_steps: 225
  logging_steps: 25
  optim: "paged_adamw_32bit"
  warmup_ratio: 0.03
  eval_strategy: "steps"
  report_to: []
