# System parameters
num_steps: 10
overseer_steps: 5
helpfulness_thresh: 4
harmlessness_thresh: 4

# User and Teacher datasets
user_data_path: './data/harmless-base/test.jsonl'
helpfulness_test_data: './data/helpful-base/test.jsonl'
helpfulness_train_data: './data/helpful-base/train.jsonl'
harmlessness_test_data: './data/harmless-base/test.jsonl'
harmlessness_train_data: './data/harmless-base/train.jsonl'

# Model parameters
# model_id: 'mistralai/Mistral-7B-Instruct-v0.1'
model_id: 'mistralai/Mistral-7B-v0.1'
lora_id: rjuggins/instruction_mistral_7b_v1_2_epochs_test
# lora_test_id: rjuggins/instruction_mistral_7b_v1_2_epochs_test
max_length: 512
hf_key_path: '../keys/hf_key.txt'

bnb_params:
  load_in_4bit: True
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_use_double_quant: False

# Outside expert parameters
expert_model_id: 'gpt-4'
openai_key_path: '../keys/openai_key.txt'

# Teacher parameters
taught_model_id: "instruction_mistral_7b_v1_2_epochs_live_dpo"
dpo_beta: 0.1

lora_params: 
  r: 32
  lora_alpha: 16
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
    - "lm_head"

teaching_params: 
  output_dir: "./models/taught_models/"
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 4
  max_grad_norm: 0.3
  gradient_checkpointing: True
  learning_rate: 2.e-5
  lr_scheduler_type: "constant"
  # max_steps: 200
  num_train_epochs: 2
  fp16: False
  bf16: True
  group_by_length: True
  save_steps: 225
  logging_steps: 25
  optim: "paged_adamw_32bit"
  warmup_ratio: 0.03
  evaluation_strategy: "steps"
  report_to: []
